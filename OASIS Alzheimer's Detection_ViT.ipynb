{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download OASIS DATASet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
      "Path to dataset files: /home/lab308/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ninadaithal/imagesoasis\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Data_size: 224 x 224\n",
    "1. Non demented: 6,7222\n",
    "2. mild demented: 5002\n",
    "3. moderate demented: 488\n",
    "4. very demented: 1,3725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "from dataset import BasicDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(size=(224, 224)),  # resize to 224x224\n",
    "    transforms.ToTensor(),          # convert PIL image to PyTorch tensor\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )  # standard ImageNet normalization\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='data/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "imagenet_loader = datasets.ImageNet(root='data/imagenet', split='train', transform=transform)\n",
    "imagenet_loader = DataLoader(imagenet_loader, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "from timm.optim import optim_factory\n",
    "from Model.ViT.models_vit import VisionTransformer\n",
    "from Model.ViT.models_mae import MaskedAutoencoderViT\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "weight_path = os.path.join(\"Model\", \"ViT\", \"mae_pretrain_vit_base.pth\")\n",
    "model_mae = MaskedAutoencoderViT(embed_dim=768, depth=12, num_heads=12)\n",
    "check_point = torch.load(weight_path)\n",
    "check_point_model = check_point['model']\n",
    "model_mae.load_state_dict(check_point_model, strict=False)\n",
    "\n",
    "model_autoencoder = VisionTransformer(embed_dim=768, depth=12, num_heads=12, num_classes=4)\n",
    "\n",
    "param_groups = optim_factory.add_weight_decay(model_mae, 0.05)\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=1.5e-4, betas=(0.9, 0.95))\n",
    "\n",
    "mseloss = nn.MSELoss()\n",
    "\n",
    "model_mae = model_mae.to(device)\n",
    "model_autoencoder = model_autoencoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token: -0.0016049178084358573\n",
      "pos_embed: 0.4599889814853668\n",
      "mask_token: -0.0013113151071593165\n",
      "decoder_pos_embed: 0.4594338536262512\n",
      "patch_embed.proj.weight: 0.00012506816710811108\n",
      "patch_embed.proj.bias: 0.013790666125714779\n",
      "blocks.0.norm1.weight: 0.2868507504463196\n",
      "blocks.0.norm1.bias: -0.007356846239417791\n",
      "blocks.0.attn.qkv.weight: -8.371970034204423e-05\n",
      "blocks.0.attn.qkv.bias: -0.020099656656384468\n",
      "blocks.0.attn.proj.weight: 7.620392716489732e-05\n",
      "blocks.0.attn.proj.bias: -0.01881934516131878\n",
      "blocks.0.norm2.weight: 0.8042056560516357\n",
      "blocks.0.norm2.bias: -0.020366661250591278\n",
      "blocks.0.mlp.fc1.weight: -0.000608887814451009\n",
      "blocks.0.mlp.fc1.bias: -1.0230910778045654\n",
      "blocks.0.mlp.fc2.weight: 4.994144183001481e-05\n",
      "blocks.0.mlp.fc2.bias: 0.033312924206256866\n",
      "blocks.1.norm1.weight: 0.5066466331481934\n",
      "blocks.1.norm1.bias: 0.0013204384595155716\n",
      "blocks.1.attn.qkv.weight: 1.9829030861728825e-05\n",
      "blocks.1.attn.qkv.bias: 0.02302500605583191\n",
      "blocks.1.attn.proj.weight: 6.507000307465205e-06\n",
      "blocks.1.attn.proj.bias: 0.01878521405160427\n",
      "blocks.1.norm2.weight: 0.7152324914932251\n",
      "blocks.1.norm2.bias: -0.005637867376208305\n",
      "blocks.1.mlp.fc1.weight: -5.662611874868162e-05\n",
      "blocks.1.mlp.fc1.bias: -1.3244516849517822\n",
      "blocks.1.mlp.fc2.weight: -2.5609660951886326e-05\n",
      "blocks.1.mlp.fc2.bias: 0.013454128056764603\n",
      "blocks.2.norm1.weight: 0.5955296754837036\n",
      "blocks.2.norm1.bias: -0.0015462622977793217\n",
      "blocks.2.attn.qkv.weight: -2.377521013841033e-05\n",
      "blocks.2.attn.qkv.bias: 0.003846831154078245\n",
      "blocks.2.attn.proj.weight: -2.333170868951129e-06\n",
      "blocks.2.attn.proj.bias: 0.013379426673054695\n",
      "blocks.2.norm2.weight: 0.7178434729576111\n",
      "blocks.2.norm2.bias: -0.012812046334147453\n",
      "blocks.2.mlp.fc1.weight: -0.00018136682047042996\n",
      "blocks.2.mlp.fc1.bias: -0.8741185069084167\n",
      "blocks.2.mlp.fc2.weight: -8.900510692910757e-06\n",
      "blocks.2.mlp.fc2.bias: 0.00628228485584259\n",
      "blocks.3.norm1.weight: 0.6752853393554688\n",
      "blocks.3.norm1.bias: -0.002609109506011009\n",
      "blocks.3.attn.qkv.weight: -4.407110463944264e-05\n",
      "blocks.3.attn.qkv.bias: 0.007699340581893921\n",
      "blocks.3.attn.proj.weight: 4.5401425268210005e-06\n",
      "blocks.3.attn.proj.bias: 0.0057349237613379955\n",
      "blocks.3.norm2.weight: 0.82375568151474\n",
      "blocks.3.norm2.bias: -0.014331812970340252\n",
      "blocks.3.mlp.fc1.weight: 1.0404312888567802e-05\n",
      "blocks.3.mlp.fc1.bias: -0.9758402705192566\n",
      "blocks.3.mlp.fc2.weight: -2.629278606036678e-05\n",
      "blocks.3.mlp.fc2.bias: 0.0031043370254337788\n",
      "blocks.4.norm1.weight: 0.7205683588981628\n",
      "blocks.4.norm1.bias: -0.006060989107936621\n",
      "blocks.4.attn.qkv.weight: -7.631425432919059e-06\n",
      "blocks.4.attn.qkv.bias: -0.0042703053914010525\n",
      "blocks.4.attn.proj.weight: 2.2751305550627876e-06\n",
      "blocks.4.attn.proj.bias: 0.010875782929360867\n",
      "blocks.4.norm2.weight: 0.8209149241447449\n",
      "blocks.4.norm2.bias: -0.012512465938925743\n",
      "blocks.4.mlp.fc1.weight: 0.00018611348059494048\n",
      "blocks.4.mlp.fc1.bias: -1.0112069845199585\n",
      "blocks.4.mlp.fc2.weight: -1.834753857110627e-05\n",
      "blocks.4.mlp.fc2.bias: 0.009637182578444481\n",
      "blocks.5.norm1.weight: 0.6646959781646729\n",
      "blocks.5.norm1.bias: -0.006189337000250816\n",
      "blocks.5.attn.qkv.weight: 1.058727502822876e-05\n",
      "blocks.5.attn.qkv.bias: -0.008770203217864037\n",
      "blocks.5.attn.proj.weight: -6.182536253618309e-06\n",
      "blocks.5.attn.proj.bias: 0.010923522524535656\n",
      "blocks.5.norm2.weight: 0.8134824633598328\n",
      "blocks.5.norm2.bias: -0.01163475215435028\n",
      "blocks.5.mlp.fc1.weight: 0.00019980460638180375\n",
      "blocks.5.mlp.fc1.bias: -1.0183727741241455\n",
      "blocks.5.mlp.fc2.weight: 1.8508644643588923e-05\n",
      "blocks.5.mlp.fc2.bias: 0.009668495506048203\n",
      "blocks.6.norm1.weight: 0.6381739377975464\n",
      "blocks.6.norm1.bias: -0.005283905193209648\n",
      "blocks.6.attn.qkv.weight: 5.608823357761139e-06\n",
      "blocks.6.attn.qkv.bias: 0.015686195343732834\n",
      "blocks.6.attn.proj.weight: 1.8734970581135713e-05\n",
      "blocks.6.attn.proj.bias: 0.008093276992440224\n",
      "blocks.6.norm2.weight: 0.8649786710739136\n",
      "blocks.6.norm2.bias: -0.01305437833070755\n",
      "blocks.6.mlp.fc1.weight: 0.00015055440599098802\n",
      "blocks.6.mlp.fc1.bias: -1.0973966121673584\n",
      "blocks.6.mlp.fc2.weight: -9.8001119113178e-06\n",
      "blocks.6.mlp.fc2.bias: 0.006196861155331135\n",
      "blocks.7.norm1.weight: 0.8509739637374878\n",
      "blocks.7.norm1.bias: -0.008926301263272762\n",
      "blocks.7.attn.qkv.weight: 2.2731312128598802e-05\n",
      "blocks.7.attn.qkv.bias: 0.0003304185520391911\n",
      "blocks.7.attn.proj.weight: -6.80329549140879e-07\n",
      "blocks.7.attn.proj.bias: 0.012467342428863049\n",
      "blocks.7.norm2.weight: 0.8879207968711853\n",
      "blocks.7.norm2.bias: -0.013080956414341927\n",
      "blocks.7.mlp.fc1.weight: 8.834302570903674e-05\n",
      "blocks.7.mlp.fc1.bias: -1.4080675840377808\n",
      "blocks.7.mlp.fc2.weight: 2.5857312721200287e-05\n",
      "blocks.7.mlp.fc2.bias: 0.014475822448730469\n",
      "blocks.8.norm1.weight: 0.8248783946037292\n",
      "blocks.8.norm1.bias: -0.00874650664627552\n",
      "blocks.8.attn.qkv.weight: 1.4104357433097903e-06\n",
      "blocks.8.attn.qkv.bias: -0.016074741259217262\n",
      "blocks.8.attn.proj.weight: 1.3025157386437058e-05\n",
      "blocks.8.attn.proj.bias: 0.01609407737851143\n",
      "blocks.8.norm2.weight: 0.9415534734725952\n",
      "blocks.8.norm2.bias: -0.017916666343808174\n",
      "blocks.8.mlp.fc1.weight: -5.2983024943387136e-05\n",
      "blocks.8.mlp.fc1.bias: -1.6087613105773926\n",
      "blocks.8.mlp.fc2.weight: -3.513293631840497e-05\n",
      "blocks.8.mlp.fc2.bias: 0.0163218192756176\n",
      "blocks.9.norm1.weight: 0.8906504511833191\n",
      "blocks.9.norm1.bias: -0.011486267670989037\n",
      "blocks.9.attn.qkv.weight: 5.19973991686129e-06\n",
      "blocks.9.attn.qkv.bias: -0.011775624938309193\n",
      "blocks.9.attn.proj.weight: 3.103971948803519e-06\n",
      "blocks.9.attn.proj.bias: 0.020257441326975822\n",
      "blocks.9.norm2.weight: 0.91457200050354\n",
      "blocks.9.norm2.bias: -0.017486633732914925\n",
      "blocks.9.mlp.fc1.weight: -2.8982143703615293e-05\n",
      "blocks.9.mlp.fc1.bias: -1.6326541900634766\n",
      "blocks.9.mlp.fc2.weight: -2.4721557565499097e-05\n",
      "blocks.9.mlp.fc2.bias: 0.01862960495054722\n",
      "blocks.10.norm1.weight: 0.861916184425354\n",
      "blocks.10.norm1.bias: -0.016123086214065552\n",
      "blocks.10.attn.qkv.weight: -4.385285592434229e-06\n",
      "blocks.10.attn.qkv.bias: -0.012127729132771492\n",
      "blocks.10.attn.proj.weight: 1.1309388355584815e-05\n",
      "blocks.10.attn.proj.bias: 0.025092696771025658\n",
      "blocks.10.norm2.weight: 0.9141179323196411\n",
      "blocks.10.norm2.bias: -0.018316928297281265\n",
      "blocks.10.mlp.fc1.weight: -9.445418254472315e-05\n",
      "blocks.10.mlp.fc1.bias: -1.5419896841049194\n",
      "blocks.10.mlp.fc2.weight: -1.1484662536531687e-06\n",
      "blocks.10.mlp.fc2.bias: 0.023085974156856537\n",
      "blocks.11.norm1.weight: 0.7898436784744263\n",
      "blocks.11.norm1.bias: -0.017403580248355865\n",
      "blocks.11.attn.qkv.weight: -2.634576230775565e-05\n",
      "blocks.11.attn.qkv.bias: 0.0018888587364926934\n",
      "blocks.11.attn.proj.weight: -3.1089195999811636e-06\n",
      "blocks.11.attn.proj.bias: 0.023419667035341263\n",
      "blocks.11.norm2.weight: 0.8497947454452515\n",
      "blocks.11.norm2.bias: -0.01811111532151699\n",
      "blocks.11.mlp.fc1.weight: -2.579222564236261e-05\n",
      "blocks.11.mlp.fc1.bias: -1.3653771877288818\n",
      "blocks.11.mlp.fc2.weight: 8.527974569005892e-05\n",
      "blocks.11.mlp.fc2.bias: 0.014541211538016796\n",
      "norm.weight: 0.72569739818573\n",
      "norm.bias: -0.03298339247703552\n",
      "decoder_embed.weight: -1.656813401496038e-05\n",
      "decoder_embed.bias: 0.0\n",
      "decoder_blocks.0.norm1.weight: 1.0\n",
      "decoder_blocks.0.norm1.bias: 0.0\n",
      "decoder_blocks.0.attn.qkv.weight: 1.400156816089293e-05\n",
      "decoder_blocks.0.attn.qkv.bias: 0.0\n",
      "decoder_blocks.0.attn.proj.weight: -0.00010844702774193138\n",
      "decoder_blocks.0.attn.proj.bias: 0.0\n",
      "decoder_blocks.0.norm2.weight: 1.0\n",
      "decoder_blocks.0.norm2.bias: 0.0\n",
      "decoder_blocks.0.mlp.fc1.weight: 4.323396206018515e-05\n",
      "decoder_blocks.0.mlp.fc1.bias: 0.0\n",
      "decoder_blocks.0.mlp.fc2.weight: 7.639191608177498e-07\n",
      "decoder_blocks.0.mlp.fc2.bias: 0.0\n",
      "decoder_blocks.1.norm1.weight: 1.0\n",
      "decoder_blocks.1.norm1.bias: 0.0\n",
      "decoder_blocks.1.attn.qkv.weight: 7.875627488829195e-06\n",
      "decoder_blocks.1.attn.qkv.bias: 0.0\n",
      "decoder_blocks.1.attn.proj.weight: 0.00010030958219431341\n",
      "decoder_blocks.1.attn.proj.bias: 0.0\n",
      "decoder_blocks.1.norm2.weight: 1.0\n",
      "decoder_blocks.1.norm2.bias: 0.0\n",
      "decoder_blocks.1.mlp.fc1.weight: 2.746232712524943e-05\n",
      "decoder_blocks.1.mlp.fc1.bias: 0.0\n",
      "decoder_blocks.1.mlp.fc2.weight: -8.55069902172545e-06\n",
      "decoder_blocks.1.mlp.fc2.bias: 0.0\n",
      "decoder_blocks.2.norm1.weight: 1.0\n",
      "decoder_blocks.2.norm1.bias: 0.0\n",
      "decoder_blocks.2.attn.qkv.weight: -6.531177496071905e-05\n",
      "decoder_blocks.2.attn.qkv.bias: 0.0\n",
      "decoder_blocks.2.attn.proj.weight: -1.1753412763937376e-05\n",
      "decoder_blocks.2.attn.proj.bias: 0.0\n",
      "decoder_blocks.2.norm2.weight: 1.0\n",
      "decoder_blocks.2.norm2.bias: 0.0\n",
      "decoder_blocks.2.mlp.fc1.weight: 4.807434743270278e-05\n",
      "decoder_blocks.2.mlp.fc1.bias: 0.0\n",
      "decoder_blocks.2.mlp.fc2.weight: 2.946266977232881e-05\n",
      "decoder_blocks.2.mlp.fc2.bias: 0.0\n",
      "decoder_blocks.3.norm1.weight: 1.0\n",
      "decoder_blocks.3.norm1.bias: 0.0\n",
      "decoder_blocks.3.attn.qkv.weight: -1.1980717317783274e-05\n",
      "decoder_blocks.3.attn.qkv.bias: 0.0\n",
      "decoder_blocks.3.attn.proj.weight: 1.2479544238885865e-05\n",
      "decoder_blocks.3.attn.proj.bias: 0.0\n",
      "decoder_blocks.3.norm2.weight: 1.0\n",
      "decoder_blocks.3.norm2.bias: 0.0\n",
      "decoder_blocks.3.mlp.fc1.weight: 3.243830724386498e-05\n",
      "decoder_blocks.3.mlp.fc1.bias: 0.0\n",
      "decoder_blocks.3.mlp.fc2.weight: -1.824413993745111e-05\n",
      "decoder_blocks.3.mlp.fc2.bias: 0.0\n",
      "decoder_blocks.4.norm1.weight: 1.0\n",
      "decoder_blocks.4.norm1.bias: 0.0\n",
      "decoder_blocks.4.attn.qkv.weight: 1.0396024663350545e-05\n",
      "decoder_blocks.4.attn.qkv.bias: 0.0\n",
      "decoder_blocks.4.attn.proj.weight: -5.654199412674643e-05\n",
      "decoder_blocks.4.attn.proj.bias: 0.0\n",
      "decoder_blocks.4.norm2.weight: 1.0\n",
      "decoder_blocks.4.norm2.bias: 0.0\n",
      "decoder_blocks.4.mlp.fc1.weight: 1.1654118679871317e-05\n",
      "decoder_blocks.4.mlp.fc1.bias: 0.0\n",
      "decoder_blocks.4.mlp.fc2.weight: -4.7031651774886996e-05\n",
      "decoder_blocks.4.mlp.fc2.bias: 0.0\n",
      "decoder_blocks.5.norm1.weight: 1.0\n",
      "decoder_blocks.5.norm1.bias: 0.0\n",
      "decoder_blocks.5.attn.qkv.weight: 2.449964449624531e-05\n",
      "decoder_blocks.5.attn.qkv.bias: 0.0\n",
      "decoder_blocks.5.attn.proj.weight: -9.41345660976367e-06\n",
      "decoder_blocks.5.attn.proj.bias: 0.0\n",
      "decoder_blocks.5.norm2.weight: 1.0\n",
      "decoder_blocks.5.norm2.bias: 0.0\n",
      "decoder_blocks.5.mlp.fc1.weight: -2.997092633449938e-05\n",
      "decoder_blocks.5.mlp.fc1.bias: 0.0\n",
      "decoder_blocks.5.mlp.fc2.weight: -3.358055619173683e-05\n",
      "decoder_blocks.5.mlp.fc2.bias: 0.0\n",
      "decoder_blocks.6.norm1.weight: 1.0\n",
      "decoder_blocks.6.norm1.bias: 0.0\n",
      "decoder_blocks.6.attn.qkv.weight: -5.8125704526901245e-05\n",
      "decoder_blocks.6.attn.qkv.bias: 0.0\n",
      "decoder_blocks.6.attn.proj.weight: 0.00010401586769148707\n",
      "decoder_blocks.6.attn.proj.bias: 0.0\n",
      "decoder_blocks.6.norm2.weight: 1.0\n",
      "decoder_blocks.6.norm2.bias: 0.0\n",
      "decoder_blocks.6.mlp.fc1.weight: -4.8421439714729786e-05\n",
      "decoder_blocks.6.mlp.fc1.bias: 0.0\n",
      "decoder_blocks.6.mlp.fc2.weight: 3.8563666748814285e-05\n",
      "decoder_blocks.6.mlp.fc2.bias: 0.0\n",
      "decoder_blocks.7.norm1.weight: 1.0\n",
      "decoder_blocks.7.norm1.bias: 0.0\n",
      "decoder_blocks.7.attn.qkv.weight: -1.6053018043749034e-06\n",
      "decoder_blocks.7.attn.qkv.bias: 0.0\n",
      "decoder_blocks.7.attn.proj.weight: -9.461818990530446e-05\n",
      "decoder_blocks.7.attn.proj.bias: 0.0\n",
      "decoder_blocks.7.norm2.weight: 1.0\n",
      "decoder_blocks.7.norm2.bias: 0.0\n",
      "decoder_blocks.7.mlp.fc1.weight: 7.041862409096211e-05\n",
      "decoder_blocks.7.mlp.fc1.bias: 0.0\n",
      "decoder_blocks.7.mlp.fc2.weight: 1.303964018006809e-05\n",
      "decoder_blocks.7.mlp.fc2.bias: 0.0\n",
      "decoder_norm.weight: 1.0\n",
      "decoder_norm.bias: 0.0\n",
      "decoder_pred.weight: 3.2440868380945176e-05\n",
      "decoder_pred.bias: 0.0\n"
     ]
    }
   ],
   "source": [
    "for name, params in model_mae.named_parameters():\n",
    "    print(f\"{name}: {params.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token\n",
      "pos_embed\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias\n",
      "blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias\n",
      "blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias\n",
      "blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias\n",
      "blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias\n",
      "blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias\n",
      "blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias\n",
      "blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias\n",
      "blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias\n",
      "blocks.4.norm1.weight\n",
      "blocks.4.norm1.bias\n",
      "blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias\n",
      "blocks.4.norm2.weight\n",
      "blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias\n",
      "blocks.5.norm1.weight\n",
      "blocks.5.norm1.bias\n",
      "blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias\n",
      "blocks.5.norm2.weight\n",
      "blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias\n",
      "blocks.6.norm1.weight\n",
      "blocks.6.norm1.bias\n",
      "blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias\n",
      "blocks.6.norm2.weight\n",
      "blocks.6.norm2.bias\n",
      "blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.bias\n",
      "blocks.6.mlp.fc2.weight\n",
      "blocks.6.mlp.fc2.bias\n",
      "blocks.7.norm1.weight\n",
      "blocks.7.norm1.bias\n",
      "blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias\n",
      "blocks.7.norm2.weight\n",
      "blocks.7.norm2.bias\n",
      "blocks.7.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias\n",
      "blocks.8.norm1.weight\n",
      "blocks.8.norm1.bias\n",
      "blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias\n",
      "blocks.8.norm2.weight\n",
      "blocks.8.norm2.bias\n",
      "blocks.8.mlp.fc1.weight\n",
      "blocks.8.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias\n",
      "blocks.9.norm1.weight\n",
      "blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias\n",
      "blocks.9.norm2.weight\n",
      "blocks.9.norm2.bias\n",
      "blocks.9.mlp.fc1.weight\n",
      "blocks.9.mlp.fc1.bias\n",
      "blocks.9.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.bias\n",
      "blocks.10.norm1.weight\n",
      "blocks.10.norm1.bias\n",
      "blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight\n",
      "blocks.10.norm2.bias\n",
      "blocks.10.mlp.fc1.weight\n",
      "blocks.10.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc2.bias\n",
      "blocks.11.norm1.weight\n",
      "blocks.11.norm1.bias\n",
      "blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias\n",
      "blocks.11.norm2.weight\n",
      "blocks.11.norm2.bias\n",
      "blocks.11.mlp.fc1.weight\n",
      "blocks.11.mlp.fc1.bias\n",
      "blocks.11.mlp.fc2.weight\n",
      "blocks.11.mlp.fc2.bias\n",
      "norm.weight\n",
      "norm.bias\n",
      "blocks.0.attn.qkv.bias\n",
      "blocks.1.attn.qkv.bias\n",
      "blocks.2.attn.qkv.bias\n",
      "blocks.3.attn.qkv.bias\n",
      "blocks.4.attn.qkv.bias\n",
      "blocks.5.attn.qkv.bias\n",
      "blocks.6.attn.qkv.bias\n",
      "blocks.7.attn.qkv.bias\n",
      "blocks.8.attn.qkv.bias\n",
      "blocks.9.attn.qkv.bias\n",
      "blocks.10.attn.qkv.bias\n",
      "blocks.11.attn.qkv.bias\n"
     ]
    }
   ],
   "source": [
    "for key in check_point_model.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoints(epoch, model, optimizer, stage):\n",
    "    checkpoint_path = f\"checkpoints/RETfound/{stage}\"\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Load checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        print(f\"Loaded checkpoint from epoch {epoch}\")\n",
    "\n",
    "def save_checkpoints(epoch, model, optimizer, stage):\n",
    "    checkpoint_path = f\"checkpoints/ConvNeXtV2/{stage}\"\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "    \n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }, checkpoint_path + f\"/checkpoint_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain fcmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrain mae on imagenet\n",
    "pretrain_epoch = 2000\n",
    "start_epoch = 0\n",
    "\n",
    "\n",
    "if start_epoch:\n",
    "    load_checkpoints(start_epoch, model_mae, optimizer, stage=\"imagenet\")\n",
    "\n",
    "for epoch in range(start_epoch, pretrain_epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    tqdm.write('Epoch: {}/{} --- < Starting Time : {} >'.format(epoch + 1,pretrain_epoch,localtime))\n",
    "    tqdm.write('-' * len('Epoch: {}/{} --- < Starting Time : {} >'.format(epoch + 1,pretrain_epoch, localtime)))\n",
    "\n",
    "    folder_name = os.path.join(\"see_image\", \"imagent\")\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for batch in tqdm(imagenet_loader):\n",
    "        img, label = batch\n",
    "        img = img.to(device)\n",
    "\n",
    "\n",
    "            \n",
    "        model_mae.train()\n",
    "        model_autoencoder.eval()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        loss, pred, mask = model_mae(img)\n",
    "            \n",
    "        image = pred[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        img_np = img[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        img_np = (img_np * 255).astype(np.uint8)\n",
    "        pred_np = (image * 255).astype(np.uint8)\n",
    "\n",
    "        cv2.imshow(\"pred img\", pred_np)\n",
    "        cv2.waitKey(1)\n",
    "        cv2.imshow(\"targe img\", img_np)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch{epoch+1} loss : \\n pretrain loss : {epoch_loss}')\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_checkpoints(epoch, model_mae, optimizer, stage=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrain mae on oasis dataset\n",
    "pretrain_epoch = 2000\n",
    "start_epoch = 0\n",
    "\n",
    "\n",
    "if start_epoch:\n",
    "    load_checkpoints(start_epoch-1, model_mae, optimizer, stage=\"pretrain\")\n",
    "\n",
    "for epoch in range(start_epoch, pretrain_epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    tqdm.write('Epoch: {}/{} --- < Starting Time : {} >'.format(epoch + 1,pretrain_epoch,localtime))\n",
    "    tqdm.write('-' * len('Epoch: {}/{} --- < Starting Time : {} >'.format(epoch + 1,pretrain_epoch, localtime)))\n",
    "\n",
    "    folder_name = os.path.join(\"see_image\", \"pre-train\")\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        img, label = batch\n",
    "        img = img.to(device)\n",
    "\n",
    "\n",
    "        loss, pred, mask = model_mae(img)\n",
    "            \n",
    "        image = pred[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        img_np = img[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        img_np = (img_np * 255).astype(np.uint8)\n",
    "        pred_np = (image * 255).astype(np.uint8)\n",
    "\n",
    "        cv2.imshow(\"pred img\", pred_np)\n",
    "        cv2.waitKey(1)\n",
    "        cv2.imshow(\"targe img\", img_np)\n",
    "        cv2.waitKey(1)\n",
    "            \n",
    "        epoch_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch{epoch+1} loss : \\n pretrain loss : {epoch_loss}')\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_checkpoints(epoch+1, model=model_mae, optimizer=optimizer, stage=\"pretrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model_autoencoder = ViTForImageClassification.from_pretrained('facebook/deit-base-patch16-224')\n",
    "model_autoencoder.classifier = nn.Linear(model_autoencoder.config.hidden_size, 4)\n",
    "model_autoencoder = model_autoencoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000 --- < Starting Time : Thu Dec 19 16:33:52 2024 >\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/169 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/169 [00:09<07:05,  2.58s/it]"
     ]
    }
   ],
   "source": [
    "# finetune classification on Oasis dataset\n",
    "pretrain_epoch = 2000\n",
    "start_epoch = 0\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "model_mae.eval()\n",
    "model_autoencoder.train()\n",
    "\n",
    "if start_epoch:\n",
    "    load_checkpoints(start_epoch-1, model_mae, optimizer, stage=\"finetune\")\n",
    "\n",
    "for epoch in range(start_epoch, pretrain_epoch):\n",
    "    torch.cuda.empty_cache()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    tqdm.write('Epoch: {}/{} --- < Starting Time : {} >'.format(epoch + 1,pretrain_epoch,localtime))\n",
    "    tqdm.write('-' * len('Epoch: {}/{} --- < Starting Time : {} >'.format(epoch + 1,pretrain_epoch, localtime)))\n",
    "\n",
    "    folder_name = os.path.join(\"see_image\", \"pre-train\")\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        img, label = batch\n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model_autoencoder(img)\n",
    "            loss = ce_loss(pred.logits, label)\n",
    "            \n",
    "        \n",
    "            \n",
    "    epoch_loss += loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    print(f'Epoch{epoch+1} loss : \\n pretrain loss : {epoch_loss}')\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        save_checkpoints(epoch+1, model=model_mae, optimizer=optimizer, stage=\"finetune\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retfound",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
