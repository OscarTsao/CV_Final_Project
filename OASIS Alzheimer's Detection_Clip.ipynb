{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download OASIS DATASet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open-eye/RETFound_MAE\n",
      "bitfount/RETFound_MAE\n",
      "bitfount/RETFound_MAE_OCT\n",
      "bitfount/RETFound_MAE_OCT_CNV_DME_DRU\n",
      "Unified/RETfound_eyepacs\n",
      "bswift/RETfound_eyepacs_DR\n",
      "sebasmos/retfound-finetuned-lora-retfound\n",
      "bitfount/RETFound_DR_IDRID\n",
      "calumburnstone/RETFoundtest\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "# List up to 10 models with \"bert\" in their name\n",
    "bert_models = list_models(search=\"retfound\", limit=10)\n",
    "for m in bert_models:\n",
    "    print(m.modelId)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab308/miniforge3/envs/clip/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "print(clip.available_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
      "Path to dataset files: /home/lab308/.cache/kagglehub/datasets/ninadaithal/imagesoasis/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ninadaithal/imagesoasis\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Data_size: 224 x 224\n",
    "1. Non demented: 6,7222\n",
    "2. mild demented: 5002\n",
    "3. moderate demented: 488\n",
    "4. very demented: 1,3725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "import clip\n",
    "from timm import optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "criteria = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "from dataset import BasicDataset\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='data/train', transform=preprocess)\n",
    "\n",
    "total_size = len(train_dataset)\n",
    "train_size = int(0.8*total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoints(epoch, model, optimizer, stage):\n",
    "    checkpoint_path = f\"checkpoints/ConvNeXtV2/{stage}\"\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Load checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        print(f\"Loaded checkpoint from epoch {epoch}\")\n",
    "\n",
    "def save_checkpoints(epoch, model, optimizer, stage):\n",
    "    checkpoint_path = f\"checkpoints/ConvNeXtV2/{stage}\"\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        os.makedirs(checkpoint_path)\n",
    "    \n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }, checkpoint_path + f\"/checkpoint_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import copy\n",
    "finetune_epoch = 10\n",
    "start_epoch = 0\n",
    "best_loss = 1000\n",
    "best_weights = copy.deepcopy(model.state_dict())\n",
    "cathegories = [\"Non Demented\", \"Mild Demeted\", \"Moderate Demented\", \"Very Mild Demented\"]\n",
    "localtime = time.asctime( time.localtime(time.time()) )\n",
    "save_model_path = os.path.join(\"save_models\", \"CLIP\")\n",
    "os.makedirs(save_model_path, exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(start_epoch, finetune_epoch):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{finetune_epoch}\", localtime)\n",
    "    print(\"-\" * len(\"Epoch {}/{}\".format(epoch+1, finetune_epoch)))\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        img, label = batch\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits_img, logits_text = model(img, clip.tokenize(cathegories).to(device))\n",
    "        probs = logits_img.softmax(dim=-1).to(torch.float32)\n",
    "        loss = criteria(probs, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Loss: {epoch_loss/len(train_loader)}\")\n",
    "\n",
    "    #Validation\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            img, label = batch\n",
    "            img, label = img.to(device), clip.tokenize(label).to(device)\n",
    "            \n",
    "\n",
    "            logits_img, logits_text = model(img, label)\n",
    "            probs = logits_img.softmax(dim=-1).detach().to(torch.float32)\n",
    "            val_loss = criteria(probs, label)\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "torch.save(best_weights, os.path.join(\"best_weights_{val_loss}_epoch_{epoch+1}.pth\"))       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [04:28<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6094\n",
      "Recall: 0.6588\n",
      "F1 Score: 0.6274\n",
      "Accuracy: 0.6588\n",
      "Test loss: 0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#validation\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "test_loss = 0\n",
    "model.eval()\n",
    "#model.load_state_dict(best_weights)\n",
    "cathegories = [\"Non Demented\", \"Mild Demeted\", \"Moderate Demented\", \"Very Mild Demented\"]\n",
    "\n",
    "# Initialize lists to store true labels and predictions\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        img, label = batch\n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "        #image_features = model.encode_image(img)\n",
    "        #text_features = model.encode_text(clip.tokenize(cathegories).to(device))\n",
    "\n",
    "        logits_img, logits_text = model(img, clip.tokenize(cathegories).to(device))\n",
    "        #probs = logits_img.softmax(dim=-1).cpu().numpy()\n",
    "        loss = criteria(logits_img, label)\n",
    "        \n",
    "        # Store the true labels and predictions\n",
    "        preds = logits_img.argmax(dim=1).cpu().numpy()\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "        all_preds.extend(preds)\n",
    "        \n",
    "# Calculate precision, recall, F1 score, and accuracy\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Test loss:\", loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convnextv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
